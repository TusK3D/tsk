{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import make_image_grid, load_image\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import PIL\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "import os\n",
    "os.putenv('HF_HOME', 'D:/CACHE')\n",
    "os.putenv('TRANSFORMERS_CACHE', 'D:/CACHE')\n",
    "os.putenv(\"HF_DATASETS_CACHE\", 'D:/CACHE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEPTH IMAGE\n",
    "imag_path = \"E:/DEV/PYTHON/NEURAL_STYLE_TRANSFER/TORCH_STABLE_DIFFUSION/riaan.jpg\"\n",
    "image = PIL.Image.open(imag_path).convert(\"RGB\").resize((512,512))\n",
    "\n",
    "def get_depth_map(image, depth_estimator):\n",
    "    image = depth_estimator(image)[\"depth\"]\n",
    "    image = np.array(image)\n",
    "    image = image[:, :, None]\n",
    "    image = np.concatenate([image, image, image], axis=2)\n",
    "    detected_map = torch.from_numpy(image).float() / 255.0\n",
    "    depth_map = detected_map.permute(2, 0, 1)\n",
    "    return depth_map\n",
    "\n",
    "depth_estimator = pipeline(\"depth-estimation\",cache_dir= 'D:/CACHE')\n",
    "depth_map = get_depth_map(image, depth_estimator).unsqueeze(0).half().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11f1p_sd15_depth\", cache_dir= 'D:/CACHE', torch_dtype=torch.float16, safety_checker=None, use_safetensors=True)\n",
    "pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", cache_dir= 'D:/CACHE', controlnet=controlnet, safety_checker=None,torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "def disabled_safety_checker(images, clip_input):\n",
    "    if len(images.shape)==4:\n",
    "        num_images = images.shape[0]\n",
    "        return images, [False]*num_images\n",
    "    else:\n",
    "        return images, False\n",
    "controlnet.safety_checker = disabled_safety_checker\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [image]\n",
    "output = pipe(\n",
    "    \"lego mini figure\", image=image, control_image=depth_map, num_inference_steps = 200, guidance_scale = 12, strength = .8\n",
    ").images\n",
    "images += output\n",
    "# make_image_grid([image, output], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pipeline outputs.\n",
    "def plot_images(images, labels = None):\n",
    "    N = len(images)\n",
    "    n_cols = 5\n",
    "    n_rows = int(np.ceil(N / n_cols))\n",
    "\n",
    "    plt.figure(figsize = (20, 5 * n_rows))\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        if labels is not None:\n",
    "            plt.title(labels[i])\n",
    "        plt.imshow(np.array(images[i]))\n",
    "        plt.axis(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(os.getcwd())\n",
    "from PIL import Image\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.device(\"cuda\")\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    device_name = torch.device(\"cpu\")\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "# model_id = \"models/controlnet11Models_tileE.safetensors\"\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = AutoPipelineForImage2Image.from_pretrained(model_id, safety_checker=None, use_safetensors=True, torch_dtype=torch.float16)\n",
    "def disabled_safety_checker(images, clip_input):\n",
    "    if len(images.shape)==4:\n",
    "        num_images = images.shape[0]\n",
    "        return images, [False]*num_images\n",
    "    else:\n",
    "        return images, False\n",
    "pipe.safety_checker = disabled_safety_checker\n",
    "\n",
    "pipe = pipe.to(\"cuda\")\n",
    "# pipe.enable_model_cpu_offload()\n",
    "\n",
    "\n",
    "imag_path = \"path/to/image\"\n",
    "image = PIL.Image.open(imag_path).convert(\"RGB\").resize((512,512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler.compatibles\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, use_karras_sigmas = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt embeddings to overcome CLIP 77 token limit.\n",
    "# https://github.com/huggingface/diffusers/issues/2136\n",
    "\n",
    "def get_prompt_embeddings(\n",
    "    pipe,\n",
    "    prompt,\n",
    "    negative_prompt,\n",
    "    split_character = \",\",\n",
    "    device = torch.device(\"cpu\")\n",
    "):\n",
    "    max_length = pipe.tokenizer.model_max_length\n",
    "    # Simple method of checking if the prompt is longer than the negative\n",
    "    # prompt - split the input strings using `split_character`.\n",
    "    count_prompt = len(prompt.split(split_character))\n",
    "    count_negative_prompt = len(negative_prompt.split(split_character))\n",
    "\n",
    "    # If prompt is longer than negative prompt.\n",
    "    if count_prompt >= count_negative_prompt:\n",
    "        input_ids = pipe.tokenizer(\n",
    "            prompt, return_tensors = \"pt\", truncation = False\n",
    "        ).input_ids.to(device)\n",
    "        shape_max_length = input_ids.shape[-1]\n",
    "        negative_ids = pipe.tokenizer(\n",
    "            negative_prompt,\n",
    "            truncation = False,\n",
    "            padding = \"max_length\",\n",
    "            max_length = shape_max_length,\n",
    "            return_tensors = \"pt\"\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "    # If negative prompt is longer than prompt.\n",
    "    else:\n",
    "        negative_ids = pipe.tokenizer(\n",
    "            negative_prompt, return_tensors = \"pt\", truncation = False\n",
    "        ).input_ids.to(device)\n",
    "        shape_max_length = negative_ids.shape[-1]\n",
    "        input_ids = pipe.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors = \"pt\",\n",
    "            truncation = False,\n",
    "            padding = \"max_length\",\n",
    "            max_length = shape_max_length\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "    # Concatenate the individual prompt embeddings.\n",
    "    concat_embeds = []\n",
    "    neg_embeds = []\n",
    "    for i in range(0, shape_max_length, max_length):\n",
    "        concat_embeds.append(\n",
    "            pipe.text_encoder(input_ids[:, i: i + max_length])[0]\n",
    "        )\n",
    "        neg_embeds.append(\n",
    "            pipe.text_encoder(negative_ids[:, i: i + max_length])[0]\n",
    "        )\n",
    "\n",
    "    return torch.cat(concat_embeds, dim = 1), torch.cat(neg_embeds, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"enter prompt here\"\"\" \n",
    "negative_prompt = \"\"\"BadDream UnrealisticDream, (deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime), text, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\"\"\" \n",
    "prompt_embeds, negative_prompt_embeds = get_prompt_embeddings(\n",
    "    pipe,\n",
    "    prompt,\n",
    "    negative_prompt,\n",
    "    split_character = \",\",\n",
    "    device = device_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompt = \"\"\"a green lego ninja, with swords, somersaults over an enemy, rooftop, lego building, background oflego city, cars, new york, ink, line drawing\"\"\"\n",
    "# # negative_prompt = \"\"\"cartoon, painting, illustration, worst quality, low quality, normal quality, shirt\"\"\"\n",
    "negative_prompt = \"\"\"\"\"\"\n",
    "\n",
    "# # from compel import Compel\n",
    "# # compel = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder)\n",
    "# # conditioning = compel.build_conditioning_tensor(prompt)\n",
    "# # prompt_embeds = conditioning\n",
    "# # conditioning_neg = compel.build_conditioning_tensor(negative_prompt)\n",
    "# # negative_prompt_embeds = conditioning_neg\n",
    "\n",
    "prompt_embeds, negative_prompt_embeds = get_prompt_embeddings(\n",
    "    pipe,\n",
    "    prompt,\n",
    "    negative_prompt,\n",
    "    split_character = \",\",\n",
    "    device = device_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to use prompt embeddings, and False to\n",
    "# use the prompt strings.\n",
    "use_prompt_embeddings = True\n",
    "# Seed and batch size.\n",
    "start_idx = 1\n",
    "batch_size = 5\n",
    "# seeds = [(4294967311+ (1e8*i)) for i  in range(start_idx , start_idx + batch_size , 1)]\n",
    "# seeds = [4294967310+(i**5) for i  in range(start_idx , start_idx + batch_size , 1)]\n",
    "seeds = [4294967315]\n",
    "\n",
    "# Number of inference steps.\n",
    "num_inference_steps = 200\n",
    "\n",
    "\n",
    "# Guidance scale.\n",
    "guidance_scale = 10\n",
    "strength = .85\n",
    "\n",
    "# Image dimensions - limited to GPU memory.\n",
    "width  = 512\n",
    "height = 512\n",
    "\n",
    "\n",
    "images = []\n",
    "\n",
    "for count, seed in enumerate(seeds):\n",
    "    start_time = time.time()\n",
    "\n",
    "    if use_prompt_embeddings is False:\n",
    "        new_img = pipe(\n",
    "            prompt = prompt,\n",
    "            negative_prompt = negative_prompt,\n",
    "            image = image,\n",
    "            width = width,\n",
    "            height = height,\n",
    "            guidance_scale = guidance_scale,\n",
    "            strength = strength,\n",
    "            num_inference_steps = num_inference_steps,\n",
    "            num_images_per_prompt = 1,\n",
    "            generator = torch.manual_seed(seed),\n",
    "        ).images\n",
    "    else:\n",
    "        new_img = pipe(\n",
    "            prompt_embeds = prompt_embeds,\n",
    "            negative_prompt_embeds = negative_prompt_embeds,\n",
    "            image = image,\n",
    "            width = width,\n",
    "            height = height,\n",
    "            guidance_scale = guidance_scale,\n",
    "            strength = strength,\n",
    "            num_inference_steps = num_inference_steps,\n",
    "            num_images_per_prompt = 1,\n",
    "            generator = torch.manual_seed(seed),\n",
    "            # Version= \"v1.5.1\",\n",
    "            # Sampler= \"DPM++ SDE Karras\",\n",
    "\n",
    "        ).images\n",
    "\n",
    "\n",
    "    images = images + new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pipeline outputs.\n",
    "def plot_images(images, labels = None):\n",
    "    N = len(images)\n",
    "    n_cols = 5\n",
    "    n_rows = int(np.ceil(N / n_cols))\n",
    "\n",
    "    plt.figure(figsize = (20, 5 * n_rows))\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        if labels is not None:\n",
    "            plt.title(labels[i])\n",
    "        plt.imshow(np.array(images[i]))\n",
    "        plt.axis(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pipeline outputs.N\n",
    "def plot_images(images, labels = None):\n",
    "    N = len(images)\n",
    "    n_cols = 5\n",
    "    n_rows = int(np.ceil(N / n_cols))\n",
    "\n",
    "    plt.figure(figsize = (20, 5 * n_rows))\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        if labels is not None:\n",
    "            plt.title(labels[i])\n",
    "        plt.imshow(np.array(images[i]))\n",
    "        plt.axis(False)\n",
    "    plt.show()\n",
    "plot_images(images, seeds[:len(images)])\n",
    "images[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdfx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
